# -*- coding: utf-8 -*-
"""
Created on Thu Jul 22 10:42:46 2021

@author: Karthic Krishnan
"""

##Bucketing Vulnerability based on Title and Threat/ Level 1 and Level 2 Category 

#load Libraries
import pandas as pd
import numpy as np
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from datetime import datetime, timedelta
from elasticsearch import helpers, Elasticsearch,RequestsHttpConnection
import ssl
from elasticsearch.connection import create_ssl_context
import warnings
warnings.filterwarnings("ignore")



#Load CMDB
cmdb = pd.read_excel("AC_Qualys_CMDB.xlsx",parse_dates=True)
cmdb.IP_ADDRESS.replace('(null)',np.nan, inplace =True)
cmdb.dropna(subset = ['IP_ADDRESS'],inplace=True)
cmdb.IP_ADDRESS.isnull().sum()
cmdb.isnull().sum()

#Load Qualys Vulnerability Management data
bucketing_vulnerability=pd.read_excel("Bucketing_Vulnerability.xlsx",sheet_name = "Data",parse_dates=True)
df1 = bucketing_vulnerability[['IP','DNS','Operating Environment', 
        'Service Criticality','Internet Facing','Severity','OS','Title','CVSS',
        'Exploit Code Maturity', 'Remediation Level',
       'Report Confidence','Titile_Level2',
       'Threat_Level2', 'Title_New_Label','Solution_Level_1',
       'Solution_Level_2','Major Pillars', 'Major Pillars1']]

#Data Preparation & metrics creation
df1.OS.replace(np.nan,'Not Available',inplace=True)
df1.DNS.replace(np.nan,'Not Available',inplace=True)
df1['Exploit Code Maturity'].replace(np.nan,'Not Available',inplace=True)
df1['Remediation Level'].replace(np.nan,'Not Available',inplace=True)
df1['Report Confidence'].replace(np.nan,'Not Available',inplace=True)
df1['vul-server_ratio'] = round(len(df1.IP)/len(pd.unique(df1['IP'])),1) 
df1['EOL_status'] = np.where(df1.Title.str.startswith('EOL'),"True","False")
bins = [-1,1,2,3,4,5,6,7,8,9,10]
labels = ['0-1','1-2','2-3','3-4','4-5','5-6','6-7','7-8','8-9','9-10']
df1['cvss_bin'] = pd.cut(df1['CVSS'], bins =bins,labels =labels)
df1['RAG'] = round(len(df1.IP)/len(pd.unique(cmdb['IP_ADDRESS'])))
df1['SAT-UNSAT'] = np.where(df1.RAG >= 98,"SAT","UNSAT")
df1['IP_Inventory'] = len(pd.unique(cmdb['IP_ADDRESS']))
df1['IP_NotScanned'] = len(pd.unique(cmdb['IP_ADDRESS'])) - len(pd.unique(df1.IP))

#data check
#df1.isnull().sum()


####Ingesting / Uploading data into Elasticsearch Index
from elasticsearch import helpers, Elasticsearch

document1 = df1.to_dict(orient='records')

es = Elasticsearch(host="158.87.122.60", port=9200,scheme="https", http_auth=('deansra','d}3@k%sbGwcJ=bcC'),verify_certs=False,timeout=600)
es.indices.delete(index='dean_sra-dev_bucketing_vulnerabilitynew1', ignore=[400, 404]) 
es.indices.create(index='dean_sra-dev_bucketing_vulnerabilitynew1',body={},ignore=[400, 404])
helpers.bulk(es, document1, index='dean_sra-dev_bucketing_vulnerabilitynew1', doc_type='docs',  ignore=400) 
print ("done")

