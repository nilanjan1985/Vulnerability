'''
    Extract the issue from 
'''
import warnings
warnings.filterwarnings('ignore', message='.*levenshtein.*')


import pandas as pd
import numpy as np
pd.set_option('mode.chained_assignment', None)
import re
from tqdm import tqdm
import os

# Language Libraries.
import nltk
from nltk.corpus import stopwords
from gensim.models.phrases import Phrases,Phraser
from gensim.corpora import Dictionary,MmCorpus
from gensim.models.ldamulticore import LdaMulticore
import math

# Sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer

from gensim.test.utils import common_texts
from gensim.models import Word2Vec


def cleanFeature(data,featureName,removeStopwords=True,lemmatize=True,minWordLength=2,myStopwords=[],myNotStopwords=[],classifyResolution=False,humanWritten=False,classifyWorkNotes=False):
    data.loc[:,featureName] = data.loc[:,featureName].str.lower()


    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'\(.*?\)','',regex=True) #Identify and eliminate values inside paranthesis.
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'(\w+\.\w+\.com)','',regex=True) #Remove servernames.
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'\w+\d+\w+|\w+\d+|\d+\w+','',regex=True) #Remove words which contain numbers in them.
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'\d+|\@|\*|\=|\+|\$|\%|\.\.|\#|-|\[|\]|\?|\"','',regex=True) #Remove special characters with nothing.
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r':|\.|\n',' ',regex=True) #Replace special characters and newline character \n with space
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'//|\,|\!|\'|\|\."','',regex=True)
    data.loc[:,featureName] = data.loc[:,featureName].str.strip()
    data.loc[:,featureName] = data.loc[:,featureName].astype('str')
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r' not ',' not_',regex=True) #Form a bigram word with not
    data.loc[:,featureName] = data.loc[:,featureName].str.replace(r' issue ','_issue ',regex=True)


    workNoteStopwords = []
    if classifyWorkNotes:
        workNoteStopwords = ['work','note','comments']

    if classifyResolution:
        data.loc[:,featureName] = data.loc[:,featureName].str.replace(r' change ',' change_') #Form a bigram word with not
        data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'(\w+) change',' change_\\1') #Form a bigram word with not
        data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'change ',' change_')

        data.loc[:,featureName] = data.loc[:,featureName].str.replace(r' replace ',' replace_') #Form a bigram word with not
        data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'(\w+) replace',' replace_\\1') #Form a bigram word with not
        data.loc[:,featureName] = data.loc[:,featureName].str.replace(r'replace ',' replace_')


    data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: x.split())
    data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: [word for word in x if word])

    humanStopwords = []
    if humanWritten:
        humanStopwords = ['change','attach','attached','time','see','try','com','thank','a_issue','an_issue','cannot',
                        'urgent','face','want','say','user','need','please','ask','call','impact','india','germany','make']
        def posFilter(token,posList = ["NN","VBD","NNP","VB","VBN"]):
            token = nltk.pos_tag(token)
            filteredWords = [word[0] for word in token if word[1] in posList]
            yield filteredWords
        tqdm.pandas()
        data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: next(posFilter(x)))


    if removeStopwords:
        stop = stopwords.words('english')
        stop.extend(['birthday','find','ibm','field','affect','establish','exe','still','from', 'subject','not_able','very','observe',
                    're','ticket','edu','regard', 'use','summary','issue','problem','get',
                    'january','february','march','april','may','june','july','august','september','october','november','december',
                    'monday','tuesday','wednesday','thursday','friday','saturday','sunday',
                    'jan','feb','mar','apr','may','jun','jul','aug','sept','oct','nov','dec',
                    'mon','tue','wed','thu','fri','sat','sun'])
        stop.extend(myStopwords)
        stop.extend(humanStopwords)
        stop.extend(workNoteStopwords)
        stop = [re.sub('\\d+','',word) for word in stop]
        notStopwords = ['down','now']
        notStopwords.extend(myNotStopwords)
        stop = [word for word in stop if word not in notStopwords]

        data.loc[:,featureName]= data.loc[:,featureName].apply(lambda x: [word for word in x if word not in stop])


    if lemmatize:
        lemma = nltk.WordNetLemmatizer()
        data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: [lemma.lemmatize(word,pos='v') for word in x]) # lemmatize Verbs
        data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: [lemma.lemmatize(word,pos='n') for word in x]) # lemmatize nouns.

    data.loc[:,featureName]=data.loc[:,featureName].apply(lambda x: [word for word in x if len(word)>minWordLength])

    if removeStopwords:
        data.loc[:,featureName]= data.loc[:,featureName].apply(lambda x: [word for word in x if word not in stop])

    return data

# ======================================
#            GENERATE N-GRAMS (PHRRASE MODELLING)
# ======================================


def generateNgrams(data,featureName,minimum_count,tHold,trigram=False,scoringMethod='default'):
    sentence_Stream = list(data.loc[:,featureName])
    phrases = Phrases(sentence_Stream,min_count=minimum_count,threshold=tHold,scoring=scoringMethod)
    bigram = Phraser(phrases)
    data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: bigram[x])
    if(trigram):
        sentence_Stream = list(data.loc[:,featureName])
        phrases = Phrases(sentence_Stream,min_count=minimum_count,threshold=tHold,scoring=scoringMethod)
        trigram = Phraser(phrases)
        data.loc[:,featureName] = data.loc[:,featureName].apply(lambda x: trigram[x])

    return data,bigram

# ======================================
#            CLASSIFY USING LDA
# ======================================

def classifyLDA2(data,featureName,bigramModel,numberOfTopics=50,confidenceThreshold = 0.2,otherThreshold = 200,model=None,returnModelOnly=False,enableLabelConfidence=True):

    listOfSentences = list(data.loc[:,featureName])
    currentDictionary = Dictionary(listOfSentences)
    currentDictionary.filter_extremes(no_below=10,no_above=0.4)
    currentDictionary.compactify()

    def corpusGenerator(listOfSnts,dictionary):
        for sentence in listOfSnts:
            yield dictionary.doc2bow(sentence)
    
    if model==None:
        MmCorpus.serialize('corpus.mm',corpusGenerator(listOfSentences,currentDictionary))
        currentCorpus = MmCorpus('corpus.mm')
        lda = LdaMulticore(currentCorpus,
                                num_topics=numberOfTopics,
                                id2word=currentDictionary,
                                workers=3)
        if returnModelOnly:
            return lda,currentDictionary
    else:
        lda=model

    if os.path.exists('corpus.mm'):
        try:
            os.remove('corpus.mm')
            os.remove('corpus.mm.index')
            print("\nDeleting corpus file\n")   
        except:
            print("\nCould not find files\n")
    else:
        print("\nCorpus Files don't exist.\n")

    


    print('=======Level 2 classification in progress =========')


    def classifyDocs2(document,unclassificationThreshold=confidenceThreshold,topWordCount=otherThreshold):
        score = lda.get_document_topics(currentDictionary.doc2bow(document))
        if len(score)==0:
            yield "__Unclassified__"
        topic,score = sorted(score,key = lambda x:x[1],reverse=True)[0]
        if score > unclassificationThreshold:
            topicLabel = " ".join([x for x,y in lda.show_topic(topic,topn=topWordCount) if x in document][0:3])
            if len(topicLabel)==0:
                topicLabel = "__Other__"
            else:
                if enableLabelConfidence:
                    #topicLabel = topicLabel+' '+['(D)','(C)','(B)','(A)'].pop(int(score/0.33)-1)
                    topicLabel = topicLabel+' '+['(B)','(A)'].pop(int(score/0.5))
            yield topicLabel+"#"+str(topic)
            #yield topicLabel

        else:
            yield "__UnderConfident__"+"#BelowThreshold"

    def reduceLevel2Labels(replacePhrase=False,replaceTokens=False,dataTemp = data):
        objective = dataTemp['level2'].value_counts()\
                                .reset_index(0)\
                                .rename(columns = {'level2':'count','index':'level2'})

        if replacePhrase:
            objectivePhrases = objective[objective['level2'].str.contains('_')]['level2']\
                                                        .apply(lambda x: [word for word in x.split() if word.find('_') != -1][0])\
                                                        .value_counts()\
                                                        .reset_index(0)\
                                                        .rename(columns = {'level2':'count','index':'level2'})
            objectivePhrases['reverse']=objectivePhrases['level2'].apply(lambda x: "_".join(x.split('_')[::-1]))

        if replaceTokens:
            objectivePhrases = objective[objective['level2'].apply(lambda x: len(x.split())==2)]
            objectivePhrases['reverse']=objectivePhrases['level2'].apply(lambda x: " ".join(x.split()[::-1]))

        objectivePhrases['renameTo']='None'
        objectivePhrases.reset_index(drop=True,inplace=True)

        def findReversePhrase(reversetokenIndex,reversetoken):
            indexFound = objectivePhrases[objectivePhrases.level2==reversetoken].index
            if len(indexFound)>0 and reversetokenIndex>indexFound[0]:
                yield objectivePhrases.level2[indexFound[0]]
            else:
                yield 'None'

        if len(objectivePhrases)>0:
            objectivePhrases['renameTo'] = [next(i) for i in np.vectorize(findReversePhrase)(objectivePhrases.reverse.index,objectivePhrases.reverse)]
            objectivePhrases = objectivePhrases[objectivePhrases['renameTo']!='None'][['level2','renameTo']].reset_index(drop=True)
            
            if replacePhrase:
                for index in objectivePhrases.index:
                    pattern = objectivePhrases["level2"][index]
                    replaceWith = objectivePhrases["renameTo"][index]
                    try:
                        dataTemp.loc[:,'level2'] = dataTemp.loc[:,'level2'].str.replace(pattern,replaceWith)
                    except:
                        print(pattern,replaceWith)
            if replaceTokens:
                for index in objectivePhrases.index:
                    pattern = objectivePhrases["level2"][index]
                    replaceWith = objectivePhrases["renameTo"][index]
                    try:
                        dataTemp.loc[:,'level2'] = dataTemp.loc[:,'level2'].str.replace(pattern,replaceWith)
                    except:
                        print(pattern,replaceWith)


        return dataTemp,objectivePhrases

    def removeRedundent(aString):
        og = aString
        aString = sorted(aString.split(),key=len)
        if len(aString)>1:
            if aString[0] in aString[-1].split('_'):
                return aString[-1]
            else:
                return og
        else:
            return og


    docList = pd.Series(listOfSentences)
    data['level2']=''                
    data.loc[:,'level2'] = docList.apply(lambda x: next(classifyDocs2(x)))
    data['clusterNumber']=data.loc[:,'level2'].apply(lambda x:x.split('#')[1])
    data.loc[:,'level2']=data.loc[:,'level2'].apply(lambda x:x.split('#')[0])

    print('=======L2 Labelling : Merging duplicate phrases =========')
    data,objective1 = reduceLevel2Labels(replacePhrase=True)

    print('=======L2 Labelling : Removing duplicate string =========')
    data.loc[:,'level2'] = data.loc[:,'level2'].apply(lambda x: removeRedundent(x))

    print('=======L2 Labelling : Merging duplicate phrases again =========')
    data,objective2 = reduceLevel2Labels(replaceTokens=True)

    print('=======Completed Level 2 Labelling =========')
    return data


def commonWordClassifier(data,clusterFeatureName,cleanDescription,significantGroupThreshold=50,reduceMore=False):
    reducedClusters = data[[clusterFeatureName]]
    test=reducedClusters
    test['level1'] ="yetToBeClassified"
    reducedClusters = reducedClusters[clusterFeatureName].str.replace('_',' ')
    reducedClusters = reducedClusters.str.strip()
    vectorizer2 = TfidfVectorizer()
    classifiedIndexs = pd.Series([False]*reducedClusters.shape[0])
    firstTime = True
    numberOfRecords = test.shape[0]
    minProgressThreshold = 0.02*numberOfRecords
    otherThresholdSize = 0.07*numberOfRecords
    while (test.level1=='yetToBeClassified').sum()>1:
        vectorizer2 = TfidfVectorizer()
        reducedClusters[classifiedIndexs] = 'alreadyClassified'
        data_vectorized2 = vectorizer2.fit_transform(reducedClusters)
        data_vectorized2 = pd.DataFrame(data_vectorized2.todense())
        data_vectorized2.columns = vectorizer2.get_feature_names()
        data_vectorized2 = data_vectorized2.append(pd.Series(data_vectorized2.sum(axis=0),name='Total'))
        if firstTime:
            maxColumnName = data_vectorized2.loc['Total',:].idxmax()
            previousRound = numberOfRecords
        else:
            maxColumnName = data_vectorized2.loc['Total',:].nlargest(2).idxmin()

        currentClassifiedIndex = (data_vectorized2[maxColumnName]>0)[0:-1]
        test['level1'][currentClassifiedIndex]=maxColumnName
        classifiedIndexs = classifiedIndexs | currentClassifiedIndex
        firstTime = False
        stillUnclassified = test[test.level1=='yetToBeClassified'].shape[0]
        progressMade = previousRound-stillUnclassified
        print(f'lowerStopLimit = {otherThresholdSize},unclassified = {stillUnclassified}, progress Made = {progressMade}')
        previousRound = stillUnclassified
        if progressMade<minProgressThreshold and stillUnclassified<otherThresholdSize:
            test.loc[test['level1']=='yetToBeClassified','level1']='Rare Groups'
    data['level1']=test['level1']

    return data


def categorize(data,
            textFeatureName,
            numberOfTopics = 25,
            mergeOnFeatureName = 'None',
            removeStopwords = True,
            humanWritten=False,
            lemmatize = True,
            minWordLength = 2,
            myStopwords = [],
            myNotStopwords = [],
            classifyResolution = False,
            classifyWorkNotes=False,
            nGramMinimumCount = 30,
            tHold = 4,
            trigram=False,
            scoringMethod='default',
            confidenceThreshold = 0,
            level2OtherGroupThreshold = 1000,
            level1RareGroupThreshold = 3,
            enableLabelConfidence=True,
            reduceMore=False):
    '''    Parameters
        ---------------

        **data** : Input Data Frame.

        **textFeatureName** : Name of the column containing Short Description or Summary or Resolution.

        **numberOfTopics = 50** : Number of topics you expect. Range from [1 to inf].

        **mergeOnFeatureName = 'None'** : If you want the output to be a part of the original dataframe, pass the name of the column containing Incident Number.

        **removeStopwords = True** : Set to either True or False.

        **lemmatize = True** : Set to either True or False.

        **minWordLength = 2** : minimum character a word must have.

        **myStopwords = []** : a customised list of stopwords Eg. ['print','clear']. It should be a list.

        **myNotStopwords = []** : a customised list of words *NOT* to be considered as stopwords. Eg. ['not','jump']

        **classifyResolution = False** : Set to True if you are classifying WorkNotes or Resolution.

        **nGramMinimumCount = 30** : Minimum Number of words required to form nGram. Range [2 to inf].

        **tHold = 4** : nGram formation Threshold. Range [0.01 to inf].

        **trigram = False** : Set to True to form trigrams.

        **scoringMethod='default'** : Other option is 'npim'

        **labelMethod = 2** : Other option is 1

        **confidenceThreshold = 0.2** : Reduce if you want the "__UnderConfidence__" group count to reduce. Range from [0 to 1]

        **level2OtherGroupThreshold = 20** : Increase if you want the "__Other__" group count to reduce. Range from [1 to 100]

        **level1RareGroupThreshold = 20** : Reduce if you want the "__RareGroup__" count to reduce. Range from [1 to inf]

        Returns
        ---------------

        A DataFrame with level1 and level2 categorization.

        .. math:: X

        .. [1] SOmething of  a reference Images are allowed, but should not be central to the explanation; users viewing the docstring as text must be able to comprehend its meaning without resorting to an image viewer. These additional illustrations are included using:

        `Example <http://www.example.com>`_

        >>> import something
        ... extending

    '''
    pd.set_option('mode.chained_assignment', None) # Supressing warnings related to loc/iloc usage.
    if mergeOnFeatureName != 'None':
        summary = data[[textFeatureName,mergeOnFeatureName]]
    else:
        summary = data[[textFeatureName]]

    summary[textFeatureName].fillna('nan',inplace=True)
    summary.loc[:,textFeatureName] = summary.loc[:,textFeatureName].str.replace('\n',' ')
    summary.reset_index(drop=True,inplace=True)

    print('=======Cleaning Data=========')
    summary1 = cleanFeature(data=summary,
                            featureName=textFeatureName,
                            removeStopwords=removeStopwords,
                            myStopwords=myStopwords,
                            myNotStopwords=myNotStopwords,
                            lemmatize=lemmatize,
                            minWordLength=minWordLength,
                            classifyResolution=classifyResolution,
                            humanWritten=humanWritten)

    print('=======Generating bigrams=========')
    summary2,bigram = generateNgrams(data=summary1,
                            featureName=textFeatureName,
                            minimum_count=nGramMinimumCount,
                            tHold=tHold,
                            trigram=trigram,
                            scoringMethod=scoringMethod)

    print('=======Creating Model =========')
    M,D = classifyLDA2(summary2,
                        textFeatureName,
                        numberOfTopics=numberOfTopics,
                        confidenceThreshold = confidenceThreshold,
                        otherThreshold=level2OtherGroupThreshold,
                        returnModelOnly=True,
                        bigramModel=bigram)


    ''' LEVEL 2 CLASSIFICATION'''
    print('=======Classifying Documents (level 2)=========')
    summary3 = classifyLDA2(summary2,
                            textFeatureName,
                            numberOfTopics=numberOfTopics,
                            confidenceThreshold = confidenceThreshold,
                            otherThreshold=level2OtherGroupThreshold,
                            model=M,
                            enableLabelConfidence=enableLabelConfidence,
                            bigramModel=bigram)
    
    ''' LEVEL 1 CLASSIFICATION'''
    print('=======Classifying Documents (level 1)=========')
    print(f"=======There are {summary3['level2'].value_counts().shape[0]} unique level 2 categories. =========")
    summary4 = commonWordClassifier(summary3,
                                'level2',
                                cleanDescription=textFeatureName,
                                significantGroupThreshold=level1RareGroupThreshold,
                                reduceMore=reduceMore)
    print('=======Merging with final Data=========')

    if mergeOnFeatureName != 'None':
        summary4 = summary4[[mergeOnFeatureName,textFeatureName,'level1','level2','clusterNumber']]
        cleanColumnName = f'clean{textFeatureName}'
        summary4 = summary4.rename(columns={textFeatureName:cleanColumnName})
        summary4 = pd.merge(data,summary4,how='left',on=mergeOnFeatureName)
    else:
        summary4 = summary4[[textFeatureName,'level1','level2','clusterNumber']]
        summary4['originalText'] = data[[textFeatureName]]
        summary4 = summary4[['originalText','level1','level2']]
    print(f"=======LDA classification is complete=======")
    return summary4

currentStopwords = ['birthday','find','ibm','field','affect','establish','exe','check','set','add','team','help','send','meet','take','look','name','org','none','failed',
                    'event','auto','chb','error','chubb','failure','fail','system','event','summary']

#currentStopwords = currentStopwords + additionalStopwords



####import data
#import data
qualys = pd.read_excel("C:\KK DRIVE\Project_GTS_ANALYTICS\DATA\R-SAC\SRAC\Vulnerability\AC_Qualys_merged\AC_Qualys_merged.xlsx")


#data check
qualys.info()
qualys.dropna(subset = ['Title'],inplace = True)
#qualys.QID.replace(np.nan,'Not Available',inplace=True)
#qualys.CMDB.replace(np.nan,'Not Available',inplace=True)
qualys.Exploitability.replace(np.nan,'Not Available',inplace=True)
qualys.reset_index(drop=True,inplace=True)

#CHECK DUPLICATES and Primary key
#len(pd.unique(qualys.QID))
n = qualys.nunique(axis=0)
print("No.of.unique values in each column :\n",n)
#len(qualys.groupby(['CMDB','QID','DNS','NetBIOS']).size().reset_index().rename(columns={0:'count'}))


#np.unique(qualys[['CMDB', 'QID']].values)
#pd.unique(qualys[['CMDB', 'QID']].values.ravel('K'))
#np.unique(qualys[['CMDB', 'QID']].values)
#pd.concat([qualys['CMDB'], qualys['QID']]).unique()
#np.unique(qualys[['CMDB', 'QID']], axis=0)


#create unique index
qualys['id'] = qualys.index
qualys['uniqueid'] = qualys.QID.astype(str) + '-' + qualys.id.astype(str)
len(pd.unique(qualys.uniqueid))

# ==============================================================================================================================================================================================
#                                                                               EXECUTE
# ==============================================================================================================================================================================================

featureName ='Solution'
idd = 'uniqueid' 
#data = qualys[[idd,featureName]]


output = categorize(data = qualys,
                    textFeatureName = featureName,
                    numberOfTopics = 5,
                    mergeOnFeatureName = idd,
                    myStopwords = currentStopwords,
                    lemmatize=True,
                    classifyResolution = False,
                    level2OtherGroupThreshold = 1000,
                    level1RareGroupThreshold = 1,
                    enableLabelConfidence = False,
                    reduceMore = True,
                    humanWritten = True,
                    classifyWorkNotes=False
        )

output.level2.value_counts().head()
output.to_excel('AC_Qualys_Solution-Topics.xlsx',index=False)
#output.to_excel('Qualys_Title-Topics.xlsx',index=False)
#output.to_excel('Qualys_Threat-Topics.xlsx',index=False)

#summary = pd.pivot_table(output,index = ['level1','level2'],values = ['level1'],aggfunc = ['count'],fill_value = 0).sort_values('count',ascending=False)






